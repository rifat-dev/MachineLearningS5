# Линейная регрессия
Линейные методы предполагают, что между признаками объекта и целевой переменной существует линейная зависимость, то есть:
$$ \hat{y} = w_1 x_1 + w_2 x_2 + ... + w_k x_k + b,$$
где $\hat{y}$ - целевая переменная (что мы хотим предсказать), $x_i$ - i-ый признак объекта $x$, $w_i$ - вес $i$-го признака, $b$ - bias (смещение, свободный член).

В задаче линейной регрессии $\hat{y}$ - это действительное число.  

# Задание 3.1
Реализуйте настройку w и b с помощью рассмотренного выше метода наименьших квадратов.  
Найдите значения метрик MSE и MAE. Сравните с результатами из sklearn.  

# Задание 3.2
Не всегда в задаче регрессии в качестве решения выступает прямая, как в предыдущем случае. Рассмотрим ещё один пример, в котором у объектов всё ещё один признак. Но теперь мы будем брать случайную точку на синусоиде и добавлять к ней шум — таким образом получим целевую переменную, признаком в этом случае будет координата $x$.  

Попробуйте реализовать настройку w и b с помощью рассмотренного выше метода наименьших квадратов.  
Найдите значения метрик MSE и MAE.  

# Задание 3.3
Реализуйте полиномиальную регрессию. Сделайте визуализацию для полиномов разных степеней.  
Полином какой степени подходит больше других? Почему?  

# Реальный датасет. Задание 3.4
Возьмём реальный набор данных Boston из sklearn.datasets. Этот датасет описывает средние цены на недвижимость в районах Бостона в тысячах долларов.  

Примеры признаков объектов недвижимости: количество преступлений на душу населения, процент старых домов в районе, количество учеников на одного учителя и т.д. Обратите внимание на то, что данные уже оцифрованы там, где изначально признаки были качественными.  

Оставьте в наборе данных только 7 наиболее значимых признаков.
Далее обучение и тестирование.
Настройте параметры линейной регрессии и сравните метрики качества (MSE и MAE) для полного датасета и усечённого.  
